# -*- coding: utf-8 -*-
"""EECS4412-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15cNCJeFDjiuA8XLqXPpBibT4XELkfN5C

## Mount to Google Drive
"""

from google.colab import drive

drive.mount('/content/gdrive/', force_remount=True)

"""## Setup"""

import numpy as np

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_datasets as tfds

import matplotlib.pyplot as plt
import pandas as pd

"""## Read the Yelp Dataset"""

df = pd.read_csv("/content/gdrive/MyDrive/train3.csv")
print((df['Class'] == 'positive').sum())
print((df['Class'] == 'negative').sum())
print((df['Class'] == 'neutral').sum())

def convert_to_number(class_name):
  if class_name == "positive":
    return 1
  elif class_name == "neutral":
    return 0
  else:
    return 2

df['Class'] = df['Class'].map(convert_to_number)
print(df.shape)

"""## Remove Stop words, punctuation and Lemmatization"""

import string
import re
import nltk

nltk.download('stopwords')

def erase_punctuation(text):
  translator = str.maketrans("", "", string.punctuation)
  return text.translate(translator)

new_stopwords = []
with open('/content/gdrive/MyDrive/stop_words.lst') as f:
    for line in f:
      new_stopwords.append(line.strip())
print(new_stopwords)

from nltk.corpus import stopwords
stpwrd = nltk.corpus.stopwords.words('english')
stpwrd.extend(new_stopwords)
print(len(stpwrd))
# remove duplicate stop words
stopwords = set(stpwrd)
print(len(stopwords))

def remove_stop(text):
  filtered_words = [word.lower() for word in text.split() if word.lower() not in stopwords]
  return " ".join(filtered_words)

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet

lemmatizer = WordNetLemmatizer()
def lemmatization(x):
  new_x = []
  for word in str(x).split():
    new_x.append(lemmatizer.lemmatize(word))
  return ' '.join(new_x)

df['Text'] = df['Text'].apply(lambda x: erase_punctuation(x))
df['Text'] = df['Text'].apply(lambda x: remove_stop(x))
df['Text'] = df['Text'].apply(lambda x: lemmatization(x))
df.head()

"""## Split the data into training, testing, and validation sets"""

import sklearn
from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

# split text and labels
train_sentences = train_df['Text'].to_numpy()
train_labels = train_df['Class'].to_numpy()

val_sentences = val_df['Text'].to_numpy()
val_labels = val_df['Class'].to_numpy()


test_sentences = test_df['Text'].to_numpy()
test_labels = test_df['Class'].to_numpy()

df.head()

print("Training entries: {}, test entries: {}".format(len(train_sentences), len(val_sentences)))

train_sentences[:10]

train_labels[:10]

"""## Make word embedding layer of NN"""

model = "https://tfhub.dev/google/nnlm-en-dim50-with-normalization/2"
hub_layer = hub.KerasLayer(model, input_shape=[], dtype=tf.string, trainable=True)
hub_layer(train_sentences[:3])

"""## Build model"""

model = tf.keras.Sequential()
model.add(hub_layer)
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(8, activation='relu'))
# model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(3, activation='softmax'))

model.summary()

model.compile(optimizer='adam',
              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),
              metrics=[tf.metrics.SparseCategoricalAccuracy(name='accuracy')])

from keras.callbacks import EarlyStopping

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)
checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

"""## Train the model"""

history = model.fit(train_sentences,
                    train_labels,
                    epochs=1000,
                    batch_size=512,
                    validation_data=(val_sentences, val_labels),
                    verbose=1,
                    callbacks=[es, model_checkpoint_callback])

"""## Evaluate the model

"""

# The model weights (that are considered the best) are loaded into the
# model.
model.load_weights(checkpoint_filepath)

results = model.evaluate(test_sentences, test_labels)

print(results)

"""## Save the model"""

model.save("/content/gdrive/MyDrive/my_model")

"""## Plot training and validation accuracy and loss over time


"""

history_dict = history.history
history_dict.keys()

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

# "bo" is for "blue dot"
plt.plot(epochs, loss, 'bo', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

plt.clf()   # clear figure

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

"""## Predict"""

predictions_df = pd.read_csv('/content/gdrive/MyDrive/test3.csv')
predictions_df

predict_sentences = predictions_df['Text'].to_numpy()
predict_x = model.predict(predict_sentences) 
classes_x = np.argmax(predict_x,axis=1)
print(len(classes_x))

unique, counts = np.unique(classes_x, return_counts=True)
dict(zip(unique, counts))

predictions_df['CLASS'] = pd.Series(classes_x)
predictions_df

def convert_num_to_class(val):
  if val == 1:
    return 'positive'
  elif val == 0:
    return 'neutral'
  else:
    return 'negative'
predictions_df['CLASS'] = predictions_df['CLASS'].map(convert_num_to_class)
predictions_df

predictions_df.rename({'ID': 'REVIEW-ID'}, axis=1, inplace=True)
predictions_df

# df.to_csv('result.csv',index=False)
header = ["REVIEW-ID", "CLASS"]
predictions_df.to_csv('/content/gdrive/MyDrive/prediction.csv', columns = header, index=False)
